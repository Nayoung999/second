{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64878073",
   "metadata": {},
   "source": [
    "  # 4-1 들어가며"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5176c184",
   "metadata": {},
   "source": [
    "# 4-2 시퀀스\n",
    "파이썬에선 값이 연속적으로 이어진 자료형들을 총칭하여 \"시퀀스 자료형(sequence type)\"이라고 부른다.\n",
    "\n",
    "\n",
    "목차\n",
    "리스트(list)\n",
    "튜플(tuple)\n",
    "셑(set)\n",
    "딕셔너리(dict)\n",
    "\n",
    "\n",
    "출처: https://kukuta.tistory.com/310 [HardCore in Programming]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec0a708",
   "metadata": {},
   "source": [
    "# list\n",
    "\n",
    "len() : 리스트 길이 리턴\n",
    "del() : 리스트 요소 삭제\n",
    "append : 리스트의 맨 뒤에 요소 추가\n",
    "insert : 리스트의 특정 인덱스에 요소 추가\n",
    "\n",
    "remove : 가장 먼저 만나는 값을 삭제\n",
    "pop : 리스트의 맨 마지막 요소 리턴 및 삭제\n",
    "extend : + 연산자와 같지만 새로운 리스트를 리턴하는 것이 아니라 extend 호출 리스트의 값이 변경 된다.\n",
    "s1 = [1, 2, 3]\n",
    "\n",
    "count, index\n",
    "\n",
    "reverse\n",
    "\n",
    "sort 메소드와 sorted 표준 함수\n",
    "\n",
    "* 리스트 복사 \n",
    "얕은 복사 : 리스트는 복사 되어 다른 객체가 만들어지지만 리스트가 가지고 있는 요소들은 공유\n",
    "\n",
    "얇은 주소는 복사 비용과 메모리 사용을 줄일 수 있지만 리스트의 요소들을 공유하므로 s1에서 발생한 변경이 의도치 않게 s3에 까지 영향을 미칠 수 있다. \n",
    "\n",
    "깊은 복사 : 리스트 객체 자체와 리스트가 가리키고 있는 요소들 모두 복사하여 새로운 인스턴스 생성\n",
    "copy 모듈의 deepcopy() 함수를 이용한다.\n",
    "\n",
    "\n",
    "출처: https://kukuta.tistory.com/310 [HardCore in Programming]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075328d4",
   "metadata": {},
   "source": [
    "# 4-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68945957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source 문장: <start> 나는 밥을 먹었다 \n",
      "Target 문장:  나는 밥을 먹었다 <end>\n"
     ]
    }
   ],
   "source": [
    "sentence = \" 나는 밥을 먹었다 \"\n",
    "\n",
    "source_sentence = \"<start>\" + sentence\n",
    "target_sentence = sentence + \"<end>\"\n",
    "\n",
    "print(\"Source 문장:\", source_sentence)\n",
    "print(\"Target 문장:\", target_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ed9c52",
   "metadata": {},
   "source": [
    "# 언어 모델\n",
    "n−1개의 단어 시퀀스 w_1, \\cdots, w_{n-1}w \n",
    "1\n",
    "​\n",
    " ,⋯,w \n",
    "n−1\n",
    "​\n",
    " 가 주어졌을 때, n번째 단어 w_nw \n",
    "n\n",
    "​\n",
    "  으로 무엇이 올지를 예측하는 확률 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acc5e34",
   "metadata": {},
   "source": [
    "# 4-4 실습 (1) 데이터 다듬기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9c6e3f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['First Citizen:', 'Before we proceed any further, hear me speak.', '', 'All:', 'Speak, speak.', '', 'First Citizen:', 'You are all resolved rather to die than to famish?', '']\n"
     ]
    }
   ],
   "source": [
    "import os, re \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# 파일을 읽기모드로 열고\n",
    "# 라인 단위로 끊어서 list 형태로 읽어옵니다.\n",
    "file_path = os.getenv('HOME') + '/aiffel/lyricist/data/shakespeare.txt'\n",
    "with open(file_path, \"r\") as f:\n",
    "    raw_corpus = f.read().splitlines()\n",
    "\n",
    "# 앞에서부터 10라인만 화면에 출력해 볼까요?\n",
    "print(raw_corpus[:9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f77060d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before we proceed any further, hear me speak.\n",
      "Speak, speak.\n",
      "You are all resolved rather to die than to famish?\n"
     ]
    }
   ],
   "source": [
    "# 1차 필터링 : 화자의 이름이나 공백 뿐인 정보를 제거\n",
    "\n",
    "for idx, sentence in enumerate(raw_corpus):\n",
    "    if len(sentence) == 0: continue   # 길이가 0인 문장은 건너뜁니다.\n",
    "    if sentence[-1] == \":\": continue  # 문장의 끝이 : 인 문장은 건너뜁니다.\n",
    "\n",
    "    if idx > 9: break   # 일단 문장 10개만 확인해 볼 겁니다.\n",
    "        \n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357c5385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰화(Tokenize) \n",
    "#: 텍스트 분류 모델에서 많이 보신 것처럼 텍스트 생성 모델에도 단어 사전을 만든다.\n",
    "# 문장을 일정한 기준으로 쪼개야하는데 그 과정을 말함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "490143fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> this is sample sentence . <end>\n"
     ]
    }
   ],
   "source": [
    "# 입력된 문장을\n",
    "#     1. 소문자로 바꾸고, 양쪽 공백을 지웁니다\n",
    "#     2. 특수문자 양쪽에 공백을 넣고\n",
    "#     3. 여러개의 공백은 하나의 공백으로 바꿉니다\n",
    "#     4. a-zA-Z?.!,¿가 아닌 모든 문자를 하나의 공백으로 바꿉니다\n",
    "#     5. 다시 양쪽 공백을 지웁니다\n",
    "#     6. 문장 시작에는 <start>, 끝에는 <end>를 추가합니다\n",
    "# 이 순서로 처리해주면 문제가 되는 상황을 방지할 수 있겠네요!\n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip() # 1\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) # 2\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 3\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence) # 4\n",
    "    sentence = sentence.strip() # 5\n",
    "    sentence = '<start> ' + sentence + ' <end>' # 6\n",
    "    return sentence\n",
    "\n",
    "# 이 문장이 어떻게 필터링되는지 확인해 보세요.\n",
    "print(preprocess_sentence(\"This @_is ;;;sample        sentence.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad53d807",
   "metadata": {},
   "source": [
    "자연어처리 분야에서 모델의 입력이 되는 문장을 \"소스 문장(Source Sentence)\"\n",
    "정답 역할을 하게 될 모델의 출력 문장을 \"타겟 문장(Target Sentence)\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1dcb518",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> before we proceed any further , hear me speak . <end>',\n",
       " '<start> speak , speak . <end>',\n",
       " '<start> you are all resolved rather to die than to famish ? <end>',\n",
       " '<start> resolved . resolved . <end>',\n",
       " '<start> first , you know caius marcius is chief enemy to the people . <end>',\n",
       " '<start> we know t , we know t . <end>',\n",
       " '<start> let us kill him , and we ll have corn at our own price . <end>',\n",
       " '<start> is t a verdict ? <end>',\n",
       " '<start> no more talking on t let it be done away , away ! <end>',\n",
       " '<start> one word , good citizens . <end>']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 여기에 정제된 문장을 모을겁니다\n",
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    # 우리가 원하지 않는 문장은 건너뜁니다\n",
    "    if len(sentence) == 0: continue\n",
    "    if sentence[-1] == \":\": continue\n",
    "    \n",
    "    # 정제를 하고 담아주세요\n",
    "    preprocessed_sentence = preprocess_sentence(sentence)\n",
    "    corpus.append(preprocessed_sentence)\n",
    "        \n",
    "# 정제된 결과를 10개만 확인해보죠\n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72e72b4",
   "metadata": {},
   "source": [
    "tf.keras.preprocessing.text.Tokenizer 패키지는 정제된 데이터를 토큰화하고,\n",
    "단어 사전(vocabulary 또는 dictionary라고 칭함)을 만들어주며, 데이터를 숫자로 변환까지 한 방에 해줍니다. 이 과정을 벡터화(vectorize) 라 하며, 숫자로 변환된 데이터를 텐서(tensor) 라고 칭합니다. \n",
    "\n",
    "\n",
    "tensor란?\n",
    "https://rekt77.tistory.com/102\n",
    "\n",
    "\n",
    "scalar 0 차원/\n",
    "vector 1 차원/\n",
    "matrix 2 차원/\n",
    "3d-tensor/\n",
    "4d-tensor/\n",
    "5d-tensor/\n",
    "6d-tensor/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c06779ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2  143   40 ...    0    0    0]\n",
      " [   2  110    4 ...    0    0    0]\n",
      " [   2   11   50 ...    0    0    0]\n",
      " ...\n",
      " [   2  149 4553 ...    0    0    0]\n",
      " [   2   34   71 ...    0    0    0]\n",
      " [   2  945   34 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7f2eb4681c10>\n"
     ]
    }
   ],
   "source": [
    "# 토큰화 할 때 텐서플로우의 Tokenizer와 pad_sequences를 사용합니다\n",
    "# 더 잘 알기 위해 아래 문서들을 참고하면 좋습니다\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences\n",
    "def tokenize(corpus):\n",
    "    # 7000단어를 기억할 수 있는 tokenizer를 만들겁니다\n",
    "    # 우리는 이미 문장을 정제했으니 filters가 필요없어요\n",
    "    # 7000단어에 포함되지 못한 단어는 '<unk>'로 바꿀거에요\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=7000, \n",
    "        filters=' ',\n",
    "        oov_token=\"<unk>\"\n",
    "    )\n",
    "    # corpus를 이용해 tokenizer 내부의 단어장을 완성합니다\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    # 준비한 tokenizer를 이용해 corpus를 Tensor로 변환합니다\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)   \n",
    "    # 입력 데이터의 시퀀스 길이를 일정하게 맞춰줍니다\n",
    "    # 만약 시퀀스가 짧다면 문장 뒤에 패딩을 붙여 길이를 맞춰줍니다.\n",
    "    # 문장 앞에 패딩을 붙여 길이를 맞추고 싶다면 padding='pre'를 사용합니다\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')  \n",
    "    \n",
    "    print(tensor,tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f181d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2  143   40  933  140  591    4  124   24  110]\n",
      " [   2  110    4  110    5    3    0    0    0    0]\n",
      " [   2   11   50   43 1201  316    9  201   74    9]]\n"
     ]
    }
   ],
   "source": [
    "#생성된 텐서 데이터를 3번째 행, 10번째 열까지만 출력해 봅시다.\n",
    "print(tensor[:3, :10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "073ac84d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : ,\n",
      "5 : .\n",
      "6 : the\n",
      "7 : and\n",
      "8 : i\n",
      "9 : to\n",
      "10 : of\n"
     ]
    }
   ],
   "source": [
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "\n",
    "    if idx >= 10: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff6b7109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2 143  40 933 140 591   4 124  24 110   5   3   0   0   0   0   0   0\n",
      "   0   0]\n",
      "[143  40 933 140 591   4 124  24 110   5   3   0   0   0   0   0   0   0\n",
      "   0   0]\n"
     ]
    }
   ],
   "source": [
    "# tensor에서 마지막 토큰을 잘라내서 소스 문장을 생성합니다\n",
    "# 마지막 토큰은 <end>가 아니라 <pad>일 가능성이 높습니다.\n",
    "src_input = tensor[:, :-1]  \n",
    "# tensor에서 <start>를 잘라내서 타겟 문장을 생성합니다.\n",
    "tgt_input = tensor[:, 1:]    \n",
    "\n",
    "print(src_input[0])\n",
    "print(tgt_input[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a9e43b",
   "metadata": {},
   "source": [
    "tf.data.Dataset객체는 텐서플로우에서 사용할 경우 데이터 입력 파이프라인을 통한 속도 개선 및 각종 편의 기능을 제공하므로 꼭 사용법을 알아 두시기를 권합니다. 우리는 이미 데이터셋을 텐서 형태로 생성해 두었으므로, tf.data.Dataset.from_tensor_slices() 메소드를 이용해 tf.data.Dataset객체를 생성할 것입니다.\n",
    "\n",
    "\n",
    "\n",
    "### 데이터 셋을 생성하기 위해 거친 과정\n",
    "정규표현식을 이용한 corpus 생성\n",
    "tf.keras.preprocessing.text.Tokenizer를 이용해 corpus를 텐서로 변환\n",
    "tf.data.Dataset.from_tensor_slices()를 이용해 corpus 텐서를 tf.data.Dataset객체로 변환\n",
    "\n",
    "\n",
    "# \n",
    "dataset을 얻음으로써 데이터 다듬기 과정은 끝났습니다. tf.data.Dataset에서 제공하는 shuffle(), batch() 등 다양한 데이터셋 관련 기능을 손쉽게 이용할 수 있게 되었군요.\n",
    "\n",
    "이 모든 일련의 과정을 텐서플로우에서의 데이터 전처리 라 칭합니다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d5828ae4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 20), (256, 20)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = len(src_input)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
    "\n",
    " # tokenizer가 구축한 단어사전 내 7000개와, 여기 포함되지 않은 0:<pad>를 포함하여 7001개\n",
    "VOCAB_SIZE = tokenizer.num_words + 1   \n",
    "\n",
    "# 준비한 데이터 소스로부터 데이터셋을 만듭니다\n",
    "# 데이터셋에 대해서는 아래 문서를 참고하세요\n",
    "# 자세히 알아둘수록 도움이 많이 되는 중요한 문서입니다\n",
    "# https://www.tensorflow.org/api_docs/python/tf/data/Dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04eb8dd",
   "metadata": {},
   "source": [
    "# 4-5 인공지능 학습시키기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "460d5e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 256\n",
    "hidden_size = 1024\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53f9b8c",
   "metadata": {},
   "source": [
    "Embedding 레이어는 이 인덱스 값을 해당 인덱스 번째의 워드 벡터로 바꿔 줍니다. 이 워드 벡터는 의미 벡터 공간에서 단어의 추상적 표현(representation)으로 사용됩니다.\n",
    "\n",
    "# \n",
    "값이 커질수록 단어의 추상적인 특징들을 더 잡아낼 수 있지만, 그만큼 충분한 데이터가 주어지지 않으면 오히려 혼란만을 야기할 수 있습니다. 이번 실습에서는 256이 적당해 보이네요\n",
    "\n",
    "# \n",
    "LSTM 레이어의 hidden state 의 차원수인 hidden_size 도 같은 맥락입니다. hidden_size 는 모델에 얼마나 많은 일꾼을 둘 것인가? 로 이해해도 크게 엇나가지 않습니다. 그 일꾼들은 모두 같은 데이터를 보고 각자의 생각을 가지는데, 역시 충분한 데이터가 주어지면 올바른 결정을 내리겠지만 그렇지 않으면 배가 산으로 갈 뿐 입니다. 이번 실습에는 1024가 적당해보이는군요.\n",
    "\n",
    "우리의 model은 아직 제대로 build되지 않았습니다. model.compile()을 호출한 적도 없고, 아직 model의 입력 텐서가 무엇인지 제대로 지정해 주지도 않았기 때문입니다.\n",
    "그런 경우 아래와 같이 model에 데이터를 아주 조금 태워 보는 것도 방법입니다. model의 input shape가 결정되면서 model.build()가 자동으로 호출됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8afe8bde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(256, 20, 7001), dtype=float32, numpy=\n",
       "array([[[-2.33152241e-06, -8.06721218e-05,  1.91171683e-04, ...,\n",
       "         -1.89930986e-04, -2.40072113e-04, -1.54353711e-05],\n",
       "        [-2.31686106e-04, -5.54067083e-04,  4.59530362e-04, ...,\n",
       "         -3.21288215e-04, -2.15684035e-04, -4.50035150e-05],\n",
       "        [-1.77370021e-04, -6.00850442e-04,  4.84908785e-04, ...,\n",
       "         -1.17042544e-03, -3.44263506e-04, -2.23926341e-04],\n",
       "        ...,\n",
       "        [ 1.91205554e-03,  2.53614224e-03, -1.95484562e-03, ...,\n",
       "         -1.27563102e-03,  2.47929333e-04,  3.77002318e-04],\n",
       "        [ 2.35863333e-03,  2.88186059e-03, -2.27866694e-03, ...,\n",
       "         -1.24726119e-03,  3.19325714e-04,  4.05193539e-04],\n",
       "        [ 2.80046114e-03,  3.13615683e-03, -2.53610732e-03, ...,\n",
       "         -1.18548819e-03,  3.67644505e-04,  4.39509313e-04]],\n",
       "\n",
       "       [[-2.33152241e-06, -8.06721218e-05,  1.91171683e-04, ...,\n",
       "         -1.89930986e-04, -2.40072113e-04, -1.54353711e-05],\n",
       "        [ 1.18130709e-04, -7.07015352e-05,  3.94069764e-04, ...,\n",
       "         -3.36726138e-04, -4.50875785e-04, -6.65257539e-05],\n",
       "        [ 3.02622066e-05,  1.71620501e-04,  4.00152087e-04, ...,\n",
       "          5.78482177e-05, -3.88145301e-04, -3.27235830e-05],\n",
       "        ...,\n",
       "        [ 2.63146171e-03,  2.44032010e-03, -2.92888982e-03, ...,\n",
       "         -1.78830011e-03, -4.10523469e-04,  7.15876813e-04],\n",
       "        [ 3.04116728e-03,  2.73698801e-03, -3.16840038e-03, ...,\n",
       "         -1.68581388e-03, -3.43093678e-04,  7.34810543e-04],\n",
       "        [ 3.44060548e-03,  2.96589802e-03, -3.34064616e-03, ...,\n",
       "         -1.55306258e-03, -2.87796895e-04,  7.53970293e-04]],\n",
       "\n",
       "       [[-2.33152241e-06, -8.06721218e-05,  1.91171683e-04, ...,\n",
       "         -1.89930986e-04, -2.40072113e-04, -1.54353711e-05],\n",
       "        [ 1.68478509e-04, -4.41363896e-04,  3.44121276e-04, ...,\n",
       "         -1.01111022e-04, -3.06439557e-04,  3.83826817e-04],\n",
       "        [ 3.56586010e-04, -4.68424289e-04,  3.06754460e-04, ...,\n",
       "         -8.30468372e-04, -3.98632634e-04,  3.97150114e-04],\n",
       "        ...,\n",
       "        [ 1.58887880e-03,  2.80703837e-03, -2.14210013e-03, ...,\n",
       "         -1.10519410e-03, -3.22952284e-04,  6.53091178e-04],\n",
       "        [ 2.05911696e-03,  3.17289052e-03, -2.49833264e-03, ...,\n",
       "         -1.24330050e-03, -2.73704121e-04,  7.15951959e-04],\n",
       "        [ 2.52946978e-03,  3.44494567e-03, -2.77647469e-03, ...,\n",
       "         -1.31000369e-03, -2.18529167e-04,  7.70346320e-04]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-2.33152241e-06, -8.06721218e-05,  1.91171683e-04, ...,\n",
       "         -1.89930986e-04, -2.40072113e-04, -1.54353711e-05],\n",
       "        [-5.93815093e-06,  1.66257538e-04,  2.28217177e-04, ...,\n",
       "         -4.83729003e-04, -6.59666723e-04, -4.72255604e-04],\n",
       "        [ 1.51427696e-04,  4.16158757e-04,  1.53917455e-04, ...,\n",
       "         -1.40775891e-03, -9.33624979e-04, -8.59222549e-04],\n",
       "        ...,\n",
       "        [ 2.07412126e-03,  3.26604559e-03, -2.68232985e-03, ...,\n",
       "         -2.14942056e-03, -8.38890788e-04,  3.25558831e-05],\n",
       "        [ 2.48184288e-03,  3.46904993e-03, -2.96300347e-03, ...,\n",
       "         -2.02582893e-03, -7.32766639e-04,  1.11962829e-04],\n",
       "        [ 2.88897683e-03,  3.60853248e-03, -3.17159435e-03, ...,\n",
       "         -1.87876460e-03, -6.30133261e-04,  1.86319769e-04]],\n",
       "\n",
       "       [[-2.33152241e-06, -8.06721218e-05,  1.91171683e-04, ...,\n",
       "         -1.89930986e-04, -2.40072113e-04, -1.54353711e-05],\n",
       "        [-1.28688072e-04,  2.82664085e-04,  3.49312410e-04, ...,\n",
       "         -1.74655972e-04,  1.00017445e-04,  2.67759053e-04],\n",
       "        [-1.66321624e-04,  4.73066058e-04,  4.19051939e-04, ...,\n",
       "         -2.59306049e-04, -3.31539522e-05,  4.64653422e-04],\n",
       "        ...,\n",
       "        [ 4.98507346e-04,  2.71251495e-03, -1.01288885e-03, ...,\n",
       "         -1.94149732e-03, -1.39112060e-03,  7.02569785e-04],\n",
       "        [ 1.02479348e-03,  2.97402195e-03, -1.45417359e-03, ...,\n",
       "         -1.88571855e-03, -1.20621570e-03,  6.72192546e-04],\n",
       "        [ 1.56435487e-03,  3.17572081e-03, -1.82353088e-03, ...,\n",
       "         -1.78863620e-03, -1.02498871e-03,  6.49720023e-04]],\n",
       "\n",
       "       [[-2.33152241e-06, -8.06721218e-05,  1.91171683e-04, ...,\n",
       "         -1.89930986e-04, -2.40072113e-04, -1.54353711e-05],\n",
       "        [-2.96085957e-04, -2.64026283e-04,  4.61423682e-04, ...,\n",
       "         -9.59578174e-05, -1.74787303e-04,  7.50849285e-05],\n",
       "        [-4.08354012e-04, -5.01219183e-04,  9.79927136e-04, ...,\n",
       "         -5.26932199e-05,  7.74611181e-05, -1.55375892e-04],\n",
       "        ...,\n",
       "        [ 3.02832946e-03,  3.40411183e-03, -2.91286572e-03, ...,\n",
       "         -1.82833103e-03,  1.37780225e-04,  6.67428074e-04],\n",
       "        [ 3.38267279e-03,  3.53598455e-03, -3.06146545e-03, ...,\n",
       "         -1.68147415e-03,  1.27558742e-04,  7.16352370e-04],\n",
       "        [ 3.72441439e-03,  3.61923082e-03, -3.17153148e-03, ...,\n",
       "         -1.51651027e-03,  1.11749534e-04,  7.59497809e-04]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터셋에서 데이터 한 배치만 불러오는 방법입니다.\n",
    "# 지금은 동작 원리에 너무 빠져들지 마세요~\n",
    "for src_sample, tgt_sample in dataset.take(1): break\n",
    "\n",
    "# 한 배치만 불러온 데이터를 모델에 넣어봅니다\n",
    "model(src_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "33f31d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  1792256   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  multiple                  5246976   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                multiple                  8392704   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  7176025   \n",
      "=================================================================\n",
      "Total params: 22,607,961\n",
      "Trainable params: 22,607,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993583d0",
   "metadata": {},
   "source": [
    "혹시라도 학습에 지나치게 많은 시간이 소요된다면 tf.test.is_gpu_available() 소스를 실행해 텐서플로우가 GPU를 잘 사용하고 있는지 확인하시길 바랍니다!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276b64e4",
   "metadata": {},
   "source": [
    "# optimizer와 loss등은 차차 배웁니다\n",
    "# 혹시 미리 알고 싶다면 아래 문서를 참고하세요\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/optimizers\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/losses\n",
    "# 양이 상당히 많은 편이니 지금 보는 것은 추천하지 않습니다\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True,\n",
    "    reduction='none'\n",
    ")\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(dataset, epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1a5f9f",
   "metadata": {},
   "source": [
    "# 4-6 잘 만들어졌는지 평가하기  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "08600428",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    # 테스트를 위해서 입력받은 init_sentence도 텐서로 변환합니다\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    # 단어 하나씩 예측해 문장을 만듭니다\n",
    "    #    1. 입력받은 문장의 텐서를 입력합니다\n",
    "    #    2. 예측된 값 중 가장 높은 확률인 word index를 뽑아냅니다\n",
    "    #    3. 2에서 예측된 word index를 문장 뒤에 붙입니다\n",
    "    #    4. 모델이 <end>를 예측했거나, max_len에 도달했다면 문장 생성을 마칩니다\n",
    "    while True:\n",
    "        # 1\n",
    "        predict = model(test_tensor) \n",
    "        # 2\n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n",
    "        # 3 \n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "        # 4\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # tokenizer를 이용해 word index를 단어로 하나씩 변환합니다 \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d88c6910",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> he s a changeling and <unk> of the <unk> , <end> '"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> he\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7479a9ed",
   "metadata": {},
   "source": [
    "# 프로젝트 : 멋진 작사가 만들기 \n",
    "\n",
    "\n",
    "\n",
    "이미 실습(1) 데이터 다듬기에서 Cloud shell에 심볼릭 링크로 ~/aiffel/lyricist/data를 생성하셨다면, ~/aiffel/lyricist/data/lyrics에 데이터가 있습니다.\n",
    "\n",
    "Step 2. 데이터 읽어오기\n",
    "glob 모듈을 사용하면 파일을 읽어오는 작업을 하기가 아주 용이해요. glob 를 활용하여 모든 txt 파일을 읽어온 후, raw_corpus 리스트에 문장 단위로 저장하도록 할게요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0c2f9e55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기: 187088\n",
      "Examples:\n",
      " [\"Now I've heard there was a secret chord\", 'That David played, and it pleased the Lord', \"But you don't really care for music, do you?\"]\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "txt_file_path = os.getenv('HOME')+'/aiffel/lyricist/data/lyrics/*'\n",
    "\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "raw_corpus = []\n",
    "\n",
    "# 여러개의 txt 파일을 모두 읽어서 raw_corpus 에 담습니다.\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "\n",
    "print(\"데이터 크기:\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:3])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
