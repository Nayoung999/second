{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a3b8427",
   "metadata": {},
   "source": [
    "# 프로젝트 : 멋진 작사가 만들기 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6cf4a3",
   "metadata": {},
   "source": [
    "## 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f901da21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기: 187088\n",
      "Examples:\n",
      " [\"Now I've heard there was a secret chord\", 'That David played, and it pleased the Lord', \"But you don't really care for music, do you?\"]\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "txt_file_path = os.getenv('HOME')+'/aiffel/lyricist/data/lyrics/*'\n",
    "\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "raw_corpus = []\n",
    "\n",
    "# 여러개의 txt 파일을 모두 읽어서 raw_corpus 에 담습니다.\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "\n",
    "print(\"데이터 크기:\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7f0653",
   "metadata": {},
   "source": [
    "## 데이터 정제 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffde1789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['First Citizen:', 'Before we proceed any further, hear me speak.', '', 'All:', 'Speak, speak.', '', 'First Citizen:', 'You are all resolved rather to die than to famish?', '']\n"
     ]
    }
   ],
   "source": [
    "import os, re \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "file_path = os.getenv('HOME') + '/aiffel/lyricist/data/shakespeare.txt'\n",
    "with open(file_path, \"r\") as f:\n",
    "    raw_corpus = f.read().splitlines()\n",
    "\n",
    "print(raw_corpus[:9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2332c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before we proceed any further, hear me speak.\n",
      "Speak, speak.\n",
      "You are all resolved rather to die than to famish?\n",
      "Resolved. resolved.\n",
      "First, you know Caius Marcius is chief enemy to the people.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for idx, sentence in enumerate(raw_corpus):\n",
    "    if len(sentence) == 0: continue   \n",
    "    if sentence[-1] == \":\": continue  \n",
    "\n",
    "    if idx > 15 : break   \n",
    "        \n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c94144d",
   "metadata": {},
   "source": [
    "##  토큰화\n",
    "다음과 같은 순서로 문장을 일정한 기준으로 쪼개는 토큰화 (Tokenize) 를 진행한다.\n",
    "\n",
    "1. 소문자로 바꾸고, 양쪽 공백을 지움\n",
    "2. 특수문자 양쪽에 공백을 넣음\n",
    "3. 여러개의 공백은 하나의 공백으로 바꿈\n",
    "4. a-zA-Z?.!,¿가 아닌 모든 문자를 하나의 공백으로 바꿈\n",
    "5. 다시 양쪽 공백을 지움\n",
    "6. 문장 시작에는 <start>, 끝에는 <end>를 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7c64f36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> this is sample sentence . <end>\n"
     ]
    }
   ],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip() # 1\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) # 2\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 3\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence) # 4\n",
    "    sentence = sentence.strip() # 5\n",
    "    sentence = '<start> ' + sentence + ' <end>' # 6\n",
    "    return sentence\n",
    "\n",
    "\n",
    "print(preprocess_sentence(\"This @_is ;;;sample        sentence.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3509475f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> before we proceed any further , hear me speak . <end>',\n",
       " '<start> speak , speak . <end>',\n",
       " '<start> you are all resolved rather to die than to famish ? <end>',\n",
       " '<start> resolved . resolved . <end>',\n",
       " '<start> first , you know caius marcius is chief enemy to the people . <end>',\n",
       " '<start> we know t , we know t . <end>',\n",
       " '<start> let us kill him , and we ll have corn at our own price . <end>',\n",
       " '<start> is t a verdict ? <end>',\n",
       " '<start> no more talking on t let it be done away , away ! <end>',\n",
       " '<start> one word , good citizens . <end>']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    # 우리가 원하지 않는 문장은 건너뜁니다\n",
    "    if len(sentence) == 0: continue\n",
    "    if sentence[-1] == \":\": continue\n",
    "    \n",
    "    # 정제를 하고 담아주세요\n",
    "    preprocessed_sentence = preprocess_sentence(sentence)\n",
    "    corpus.append(preprocessed_sentence)\n",
    "        \n",
    "# 정제된 결과를 10개만 확인해보죠\n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22b9c2a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2  143   40 ...    0    0    0]\n",
      " [   2  110    4 ...    0    0    0]\n",
      " [   2   11   50 ...    0    0    0]\n",
      " ...\n",
      " [   2  149 4553 ...    0    0    0]\n",
      " [   2   34   71 ...    0    0    0]\n",
      " [   2  945   34 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7fc20d239a90>\n"
     ]
    }
   ],
   "source": [
    "def tokenize(corpus):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "    num_words=12000,   # 단어장의 크기는 12,000 이상으로 설정\n",
    "    filters=' ',\n",
    "    oov_token=\"<unk>\"\n",
    "    )\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)   \n",
    "    \n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')  \n",
    "    \n",
    "    print(tensor,tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df928150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2  143   40  933  140  591    4  124   24  110]\n",
      " [   2  110    4  110    5    3    0    0    0    0]\n",
      " [   2   11   50   43 1201  316    9  201   74    9]]\n"
     ]
    }
   ],
   "source": [
    "print(tensor[:3, :10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae4803be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : ,\n",
      "5 : .\n",
      "6 : the\n",
      "7 : and\n",
      "8 : i\n",
      "9 : to\n",
      "10 : of\n",
      "11 : you\n",
      "12 : my\n",
      "13 : a\n",
      "14 : that\n",
      "15 : ?\n"
     ]
    }
   ],
   "source": [
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "\n",
    "    if idx >= 15: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "192010ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2 143  40 933 140 591   4 124  24 110   5   3   0   0   0   0   0   0\n",
      "   0   0]\n",
      "[143  40 933 140 591   4 124  24 110   5   3   0   0   0   0   0   0   0\n",
      "   0   0]\n"
     ]
    }
   ],
   "source": [
    "src_input = tensor[:, :-1]  # 소스 문장 생성\n",
    "\n",
    "tgt_input = tensor[:, 1:]    # 타겟 문장 생성\n",
    "\n",
    "print(src_input[0])\n",
    "print(tgt_input[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "748bc843",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((12000, 20), (12000, 20)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = len(src_input)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
    "\n",
    "VOCAB_SIZE = tokenizer.num_words + 1   \n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1a5944d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, \n",
    "                                                          tgt_input,\n",
    "                                                          test_size=0.2,       # 총 데이터의 20%를 평가 데이터셋으로 사용\n",
    "                                                          shuffle=True, \n",
    "                                                          random_state=34)     # 결과를 일정하게 보여주기위해 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "51406e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Train: (19212, 20)\n",
      "Target Train: (19212, 20)\n"
     ]
    }
   ],
   "source": [
    "print(\"Source Train:\", enc_train.shape)\n",
    "print(\"Target Train:\", dec_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34d1ec0",
   "metadata": {},
   "source": [
    "19212개의 학습 데이터를 확인할 수 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ba3a5c",
   "metadata": {},
   "source": [
    "### 모델  학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "951d48e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 256\n",
    "hidden_size = 1024\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f92cd28",
   "metadata": {},
   "source": [
    "TextGenerator는 1개의 embedding layer, 2개의 rnn(LSTM) layer, 1개의 Dense layer로 구성된다. \n",
    "\n",
    "embedding_size = word vector의 차원 수, 단어가 추상적으로 표현되는 크기.\n",
    "hidden_size =  LSTM 레이어의 hidden state의 차원수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9c75b453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "601/601 [==============================] - 107s 139ms/step - loss: 2.7053\n",
      "Epoch 2/10\n",
      "601/601 [==============================] - 84s 140ms/step - loss: 2.3846\n",
      "Epoch 3/10\n",
      "601/601 [==============================] - 84s 140ms/step - loss: 2.2284\n",
      "Epoch 4/10\n",
      "601/601 [==============================] - 84s 140ms/step - loss: 2.0811\n",
      "Epoch 5/10\n",
      "601/601 [==============================] - 84s 140ms/step - loss: 1.9216\n",
      "Epoch 6/10\n",
      "601/601 [==============================] - 84s 140ms/step - loss: 1.7395\n",
      "Epoch 7/10\n",
      "601/601 [==============================] - 84s 140ms/step - loss: 1.5375\n",
      "Epoch 8/10\n",
      "601/601 [==============================] - 84s 140ms/step - loss: 1.3302\n",
      "Epoch 9/10\n",
      "601/601 [==============================] - 84s 140ms/step - loss: 1.1308\n",
      "Epoch 10/10\n",
      "601/601 [==============================] - 84s 140ms/step - loss: 0.9550\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc14c2ff3a0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True,\n",
    "    reduction='none'\n",
    ")\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(enc_train, dec_train, epochs=10) # 10 epoch. val_loss =2.2를 줄일 수 있도록"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639213cd",
   "metadata": {},
   "source": [
    "### Step 5. 인공지능 만들기\n",
    "모델의 Embedding Size와 Hidden Size를 조절하며 10 Epoch 안에 val_loss 값을 2.2 수준으로 줄일 수 있는 모델을 설계하세요! (Loss는 아래 제시된 Loss 함수를 그대로 사용!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "aadcd5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=15):\n",
    "    \n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "     \n",
    "    while True:\n",
    "        predict = model(test_tensor)  \n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1]    \n",
    "\n",
    "         \n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "\n",
    "        \n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0f41910e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loss\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "43e8345e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i love thee not a jar o the clock behind <end> '"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i love\", max_len=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0d2d2597",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> show d mastership in floating fortune s blows , <end> '"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> show\", max_len=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e0f337ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> you have made worms meat of me i have it , <end> '"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> you\", max_len=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "81661403",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> mouse in their election . <end> '"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> mouse\", max_len=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9d3a341e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> water to be <unk> with our <unk> , <end> '"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> water\", max_len=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "400a6cc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> deep learning , fair <unk> , of all eyes , <end> '"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> deep learning\", max_len=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9a861bde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> <unk> , and potpan ! <end> '"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> hey\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "27454f8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> <unk> , and potpan ! <end> '"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> mom\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7a130424",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> <unk> , and potpan ! <end> '"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> Siri\", max_len=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3f201b",
   "metadata": {},
   "source": [
    "# 회고\n",
    "\n",
    "1. 포함되지 않는 단어는 <unk>로 표현된다고 하는데 생각보다 많이 나온다. \n",
    "2. 어려웠던 부분은 optimizer = tf.keras.optimizers.Adam() 와 loss = tf.keras.losses.SparseCategoricalCrossentropy를 사용하는 부분이었다. 이번 노드에서 보이지 않았고 수정해야하는 부분을 찾는 것이 쉽지 않았다. \n",
    "3. tokenize를 할 때 oov_token이 뭔지 몰랐다. oov= out of vocabulary\n",
    "4. embedding_size와 hidden_size를 다양하게 도전해보고 싶었으나 학습에 시간이 너무 많이 걸려 할 수 없었다. \n",
    "5. 지금은 학습 데이터로 사용된 자료들이 영어로 된 노래였다. 하지만, 한글로 된 자료를 이용해서도 만들어 보고 싶다. 한국어는 어순이 달라도 의미가 크게 달라지지 않아, 이상하더라도 듣는 사람이 알아 들을 수 있다. 물론, 시적 허용으로 이해 될 수도 있다. 이런 점에서 한국의 시를 이용해서 만들어 보고 싶다.\n",
    "    - 시와 노래 가사는 크게 다른 바가 없다고 생각한다. 하지만, 시에는 시인의 생활 환경이나, 그 사람의 역사에 대한 내용이 담기기 마련이다. \n",
    "    - 특히, 지금 갑자기 \"역사, 사회의 배경이 동일한 시대의 작품을 학습시키면 그 시대에 살았던 시인들과 비슷한 작품을 만들어 낼 수 있을까?\"라는 의문이 생겼다. 우리나라는 100여 년간 일제 강점기와 광복, 6.25전쟁, 민주화 운동 등 그리 길지 않은 시간에 많은 굴곡진 역사가 있었고, 그 시대적 상황의 변화에 따라 많은 작품이 형성되었다. 시어 속에 숨겨진 중의적인 단어의 의미나 상징적인 단어들이 딥러닝 학습을 통해서 제대로 구현될 수 있을 지 궁금하다. \n",
    "\n",
    "6. 화가들의 그림을 학습해서 그 화가의 스타일로 그림을 그리는 AI에 대한 기사를 본 적이 있다. \n",
    "    https://nownews.seoul.co.kr/news/newsView.php?id=20211012601012\n",
    "    \n",
    "   그렇다면 작가의 글을 학습해서 그 작가의 스타일로 작품을 생성하는 것도 가능하리라 짐작한다.\n",
    "    \n",
    "   최근 상영했던 SF영화 '듄(dune)'은 동명의 소설을 원작으로 하고 있는데, 6편의 소설을 집필하고 작가는 생을 마감하게 되었다. 다른 작가나 아들이 소설을 완결짓기 위해서 노력했으나 원작자에 못 미치는 수준이라 독자들의 아쉬움이 크다고 했다. 그렇다면, AI로 원작자의 스타일을 학습시킨다면 원작자의 아이디어와 가장 근접한 작품을 만들어 낼 수 있지 않을까하는 기\n",
    "    \n",
    "7. 생성된 모든 문장이 자연스러운 것은 아니다. 그리고 다른  init_sentesnce를 넣었지만 알 수 없는 이유로 \"and potpan !\"이 계속 나오기도 한다. you have made worms meat of me i have it.(넌 나를 나를 벌레 고기로 만들었다..)라는 이상한 가사가 만들어지기도 했다. \n",
    "    \n",
    "'<start> deep learning , fair <unk> , of all eyes , <end> '\n",
    "    -> 꿈보다 해몽이라고 했던가. 꽤나 철학적인 가사가 탄생했다.  init sentence로 'deep learning'을 넣었는데 이렇게 철학적인 노래 가사가 탄생했다. Deep learning, fair of all eyes. 딥러닝, 공평한 모두의 눈. 우연의 조합이지만 딥러닝에 대한 정확한 통찰을 보여주는 가사가 만들어졌다. \n",
    "    \n",
    "    => 학습할 데이터의 양을 아주 많이 늘리면 어색하거나 이상한 내용의 문제는 점차 줄어들 것이다. \n",
    "    \n",
    "8. 이번 '작사가 인공지능 만들기' 프로젝트를 통해서 AI를 응용할 수 있는 다양한 방법들을 생각할 수 있었다.\n",
    "    특히 다른 프로젝트에 비해서 응용해보고 싶은 마음이 많이 들었다. \n",
    "    \n",
    "9. 4번째 epoch부터 loss는 2.2 이하로 낮아졌다. 최종 10번째 epoch에서는 0.9550의 아주 낮은 loss를 보였다. \n",
    "    그런데 여기서 의문이 든다. 지금 loss가 낮아진 결과 문장의 완성도가 높아진 것이 지금 이 수준인가? 그렇다면 epoch만이라도... 계속 늘려보고 싶다. 다음 기회에 계속."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
